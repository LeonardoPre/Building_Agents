{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://api.smith.langchain.com\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "print(os.getenv(\"LANGSMITH_ENDPOINT\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import os\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "\n",
    "\n",
    "llm = init_chat_model(\n",
    "    \"azure_openai:gpt-4.1-mini\",\n",
    "    azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\n",
    "    api_version=\"2024-12-01-preview\"\n",
    ")\n",
    "\n",
    "agent_tools = [tools.get_all_plants, tools.get_bloom_data_whole]\n",
    "llm_with_tools = llm.bind_tools(agent_tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['apfel', 'birke', 'erle', 'hasel', 'winterraps', 'winterroggen']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['apfel_agent',\n",
       " 'birke_agent',\n",
       " 'erle_agent',\n",
       " 'hasel_agent',\n",
       " 'winterraps_agent',\n",
       " 'winterroggen_agent']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils import make_specialist\n",
    "\n",
    "plants = tools.get_all_plants().split(\" \")\n",
    "print(plants)\n",
    "#create specialists\n",
    "specialists = {plant: make_specialist(plant, llm) for plant in plants}\n",
    "[specialist.name for specialist in specialists.values()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: Heute, am 11.04.2021, befinden wir uns in Berlin. Ist die Apfel gerade am Blühen?\n",
      "Tool invoked with plant=apfel, year=2021, type=beginn\n",
      "Tool invoked with plant=apfel, year=2021, type=ende\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Heute, am 11.04.2021, befinden wir uns in Berlin. Ist die Apfel gerade am Blühen?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Name: apfel_agent\n",
      "Tool Calls:\n",
      "  get_bloom_data_apfel (call_rk3IccoNNuD61MVOtnjt0a3M)\n",
      " Call ID: call_rk3IccoNNuD61MVOtnjt0a3M\n",
      "  Args:\n",
      "    year: 2021\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: get_bloom_data_apfel\n",
      "\n",
      "{\"begin_data\": {\"Jahr\": {\"4\": 2021}, \"Gebietsmittel\": {\"4\": \"01.05. \"}, \"Abweichung vom Mittel\": {\"4\": \"5 Tage\"}}, \"ende_data\": {\"Jahr\": {\"4\": 2021}, \"Mittel 78 %\": {\"4\": \"22.05. \"}, \"Abweichung 78 %\": {\"4\": \"9 Tage \"}, \"Mittel 100 %\": {\"4\": \"26.05. \"}, \"Abweichung 100 %\": {\"4\": \"10 Tage\"}}, \"info_beginn\": \"Tabelle mit den mittleren Beobachtungsterminen und den Abweichungen vom Mittel der Phase \\\"Apfel: Blüh-Beginn\\\" im Gebiet Deutschland für verschiedene Jahre. Positive Abweichungen bedeuten, dass die Phase im entsprechendem Jahr später aufgetreten ist.\\n\\n© Deutscher Wetterdienst, erstellt 23.05.2025 01:34 UTC. Alle Angaben ohne Gewähr!\", \"info_ende\": \"Tabelle mit den mittleren Beobachtungsterminen und den Abweichungen vom Mittel der Phase \\\"Apfel: Blüh-Ende im Beobachtungsgebiet\\\" im Gebiet Deutschland für verschiedene Jahre. Es werden die Gebietsmittel und Abweichungen bei der aktuellen Meldequote von 78 % ausgegeben, des Weiteren die Gebietsmittel und Abweichungen der Vorjahre bei einer Meldequote von 100 % (abgeschlossene Phase). Positive Abweichungen bedeuten, dass die Phase im entsprechendem Jahr später aufgetreten ist.\\n\\n© Deutscher Wetterdienst, erstellt 22.06.2025 01:25 UTC. Alle Angaben ohne Gewähr!\"}\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Name: apfel_agent\n",
      "\n",
      "{\n",
      "  \"response\": false,\n",
      "  \"reasoning\": \"Die Apfelblüte begann in Berlin im Jahr 2021 laut den Daten im Durchschnitt am 01.05.2021, mit einer Abweichung von 5 Tagen, was bedeutet, dass der Blühbeginn etwa um den 6. Mai war. Da heute der 11.04.2021 ist, hat die Apfelblüte noch nicht begonnen.\"\n",
      "}\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "slice(None, -3, None)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 27\u001b[39m\n\u001b[32m     25\u001b[39m         cleaned_response = response.strip(\u001b[33m\"\u001b[39m\u001b[33m```json\u001b[39m\u001b[33m\"\u001b[39m).strip(\u001b[33m\"\u001b[39m\u001b[33m```\u001b[39m\u001b[33m\"\u001b[39m).strip()\n\u001b[32m     26\u001b[39m         response_json = json.loads(cleaned_response)\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m     tokens = get_tokens(\u001b[43mmessages\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[32m     28\u001b[39m     responses.append({\n\u001b[32m     29\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mid\u001b[39m\u001b[33m\"\u001b[39m: data[\u001b[33m\"\u001b[39m\u001b[33mid\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m     30\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mplant\u001b[39m\u001b[33m\"\u001b[39m: data[\u001b[33m\"\u001b[39m\u001b[33mplant\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m   (...)\u001b[39m\u001b[32m     36\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mtokens\u001b[39m\u001b[33m\"\u001b[39m: tokens\n\u001b[32m     37\u001b[39m     })\n\u001b[32m     41\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mresults/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00magent.name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_24.json\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mw\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n",
      "\u001b[31mKeyError\u001b[39m: slice(None, -3, None)"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "import json\n",
    "from utils import get_tokens\n",
    "#base_prompt = 'Antworte in JSON und folge dem Schema: { \"type\": \"object\", \"properties\": { \"response\": { \"type\": \"boolean\" }, \"reasoning\": { \"type\": \"string\" } }, \"required\": [ \"response\", \"reasoning\" ] }'\n",
    "with open(\"datasets/dataset_24.json\", \"r\") as file:\n",
    "    dataset = json.load(file)\n",
    "\n",
    "\n",
    "responses = []\n",
    "for agent in specialists.values():\n",
    "    for data in dataset[:1]:\n",
    "        sentence = data[\"sentence\"]\n",
    "        prompt = [HumanMessage(content=sentence)]\n",
    "        print(f\"Processing: {sentence}\")\n",
    "        messages = agent.invoke({\"messages\": prompt})\n",
    "        for m in messages['messages']:\n",
    "            m.pretty_print()\n",
    "        response = messages[\"messages\"][-1].content\n",
    "        cleaned_response = response.strip(\"```json\").strip(\"```\").strip()\n",
    "        response_json = json.loads(cleaned_response)\n",
    "        tokens = get_tokens(messages)\n",
    "        responses.append({\n",
    "            \"id\": data[\"id\"],\n",
    "            \"plant\": data[\"plant\"],\n",
    "            \"city\": data[\"city\"],\n",
    "            \"date\": data[\"date\"],\n",
    "            \"sentence\": sentence,\n",
    "            \"response\": response_json[\"response\"],\n",
    "            \"reasoning\": response_json[\"reasoning\"],\n",
    "            \"tokens\": tokens\n",
    "        })\n",
    "\n",
    "\n",
    "\n",
    "    with open(f\"results/{agent.name}_24.json\", \"w\") as file:\n",
    "        json.dump(responses, file, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".ocits-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
